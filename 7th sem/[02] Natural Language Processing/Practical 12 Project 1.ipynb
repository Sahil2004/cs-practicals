{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPx1ABH7u7WzMfn+bHWXb/t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahil2004/cs-practicals/blob/main/7th%20sem/%5B02%5D%20Natural%20Language%20Processing/Practical%2012%20Project%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gBIl5gIpajnF",
        "outputId": "8444039a-bc1c-4366-8978-a6fa72dcc2ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy nltk\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "8aNsJ27ocRET"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oTnRXOupcTCx",
        "outputId": "69c3234e-3fb2-45a5-c078-75fc4865bd47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "EvTSyEF3cT1R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The students are studying Natural Language Processing in modern computer systems.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MR9Je0K_cWvQ",
        "outputId": "5e62e0af-e413-4f63-d799-bbb06f8bef51"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "The students are studying Natural Language Processing in modern computer systems.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [token.text for token in doc if not token.is_punct]\n",
        "\n",
        "print(\"\\nTokens:\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QjijDqNecYis",
        "outputId": "247825b4-ad87-4890-f1fa-cca88126044c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokens:\n",
            "['The', 'students', 'are', 'studying', 'Natural', 'Language', 'Processing', 'in', 'modern', 'computer', 'systems']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "print(\"\\nStemmed Words:\")\n",
        "print(stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-I4JtqWScaWt",
        "outputId": "8c83052a-e1f9-468b-b5f8-c958e7cdc271"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemmed Words:\n",
            "['the', 'student', 'are', 'studi', 'natur', 'languag', 'process', 'in', 'modern', 'comput', 'system']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas = [token.lemma_ for token in doc if not token.is_punct]\n",
        "\n",
        "print(\"\\nLemmatized Words:\")\n",
        "print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FwbjTK-RcchP",
        "outputId": "6c3072f4-6efa-403e-c23b-17acf9b7c1b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatized Words:\n",
            "['the', 'student', 'be', 'study', 'Natural', 'Language', 'processing', 'in', 'modern', 'computer', 'system']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = [(token.text, token.pos_) for token in doc if not token.is_punct]\n",
        "\n",
        "print(\"\\nPOS Tags:\")\n",
        "for word, pos in pos_tags:\n",
        "    print(f\"{word} → {pos}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "n0UVoeghceKm",
        "outputId": "e5b419e5-8ca1-4fe7-fcb6-8d2eacf37d68"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS Tags:\n",
            "The → DET\n",
            "students → NOUN\n",
            "are → AUX\n",
            "studying → VERB\n",
            "Natural → PROPN\n",
            "Language → PROPN\n",
            "Processing → NOUN\n",
            "in → ADP\n",
            "modern → ADJ\n",
            "computer → NOUN\n",
            "systems → NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams = list(ngrams(tokens, 2))\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "\n",
        "print(\"\\nBigrams:\")\n",
        "print(bigrams)\n",
        "\n",
        "print(\"\\nTrigrams:\")\n",
        "print(trigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Chto_4c-cf_3",
        "outputId": "f6086362-945d-43f6-b989-266261b9d1f9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bigrams:\n",
            "[('The', 'students'), ('students', 'are'), ('are', 'studying'), ('studying', 'Natural'), ('Natural', 'Language'), ('Language', 'Processing'), ('Processing', 'in'), ('in', 'modern'), ('modern', 'computer'), ('computer', 'systems')]\n",
            "\n",
            "Trigrams:\n",
            "[('The', 'students', 'are'), ('students', 'are', 'studying'), ('are', 'studying', 'Natural'), ('studying', 'Natural', 'Language'), ('Natural', 'Language', 'Processing'), ('Language', 'Processing', 'in'), ('Processing', 'in', 'modern'), ('in', 'modern', 'computer'), ('modern', 'computer', 'systems')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
        "\n",
        "print(\"\\nNoun Phrases:\")\n",
        "print(noun_phrases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "errhvrtvchi8",
        "outputId": "a297068a-c584-4946-e9e1-7953a2cad16b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Noun Phrases:\n",
            "['The students', 'Natural Language Processing', 'modern computer systems']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"processing\"\n",
        "synsets = wordnet.synsets(word)\n",
        "\n",
        "print(\"\\nLexical Information for word:\", word)\n",
        "for syn in synsets[:3]:\n",
        "    print(\"-\", syn.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sXuojWBpcjrC",
        "outputId": "4c188083-bc8e-4cd9-b9af-d0414a2fbcc5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lexical Information for word: processing\n",
            "- preparing or putting through a prescribed procedure\n",
            "- subject to a process or treatment, with the aim of readying for some purpose, improving, or remedying a condition\n",
            "- deal with in a routine way\n"
          ]
        }
      ]
    }
  ]
}